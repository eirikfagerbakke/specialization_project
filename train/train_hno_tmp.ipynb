{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one device detected. Disabling array sharding and autoparallelism.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "import jax_dataloader as jdl\n",
    "from utils import *\n",
    "\n",
    "import jax.experimental.mesh_utils as mesh_utils\n",
    "import jax.sharding as jshard\n",
    "import argparse\n",
    "from optax.contrib import reduce_on_plateau\n",
    "\n",
    "problem = \"advection\"\n",
    "network = \"deeponet\"\n",
    "running_on = \"local\"\n",
    "num_epochs = 5\n",
    "use_hino = True\n",
    "\n",
    "if running_on == \"local\":\n",
    "    data_path = \"C:/Users/eirik/OneDrive - NTNU/5. klasse/prosjektoppgave/eirik_prosjektoppgave/data/\"\n",
    "    hparams_path = \"C:/Users/eirik/OneDrive - NTNU/5. klasse/prosjektoppgave/eirik_prosjektoppgave/hyperparameters/\"\n",
    "    checkpoint_path = \"C:/Users/eirik/orbax/checkpoints/\"\n",
    "elif running_on == \"idun\":\n",
    "    data_path = \"/cluster/work/eirikaf/data/\"\n",
    "    hparams_path = \"/cluster/home/eirikaf/phlearn-summer24/eirik_prosjektoppgave/hyperparameters/\"\n",
    "    checkpoint_path = \"/cluster/work/eirikaf/checkpoints/\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid running_on\")\n",
    "\n",
    "scaled_data = jnp.load(data_path + problem + \"_scaled_data.npz\")\n",
    "a_train_s = jnp.array(scaled_data[\"a_train_s\"])\n",
    "u_train_s = jnp.array(scaled_data[\"u_train_s\"])\n",
    "a_val_s = jnp.array(scaled_data[\"a_val_s\"])\n",
    "u_val_s = jnp.array(scaled_data[\"u_val_s\"])\n",
    "\n",
    "x_train_s = jnp.array(scaled_data[\"x_train_s\"])\n",
    "t_train_s = jnp.array(scaled_data[\"t_train_s\"])\n",
    "\n",
    "# DATALOADERS\n",
    "train_loader = jdl.DataLoader(jdl.ArrayDataset(a_train_s, u_train_s, asnumpy = False), batch_size=16, shuffle=True, backend='jax', drop_last=True)\n",
    "val_loader = jdl.DataLoader(jdl.ArrayDataset(a_val_s, u_val_s, asnumpy = False), batch_size=16, shuffle=True, backend='jax', drop_last=True)\n",
    "\n",
    "# AUTOPARALLELISM\n",
    "sharding_a, sharding_u, replicated = create_device_mesh()\n",
    "\n",
    "# IMPORT WANTED NETWORK ARCHITECTURE\n",
    "if network == \"deeponet\":\n",
    "    from networks.deeponet import DeepONet as OperatorNet\n",
    "    from networks.deeponet import Hparams as OperatorHparams\n",
    "    if use_hino:\n",
    "        from networks.hino_DON import *\n",
    "        from networks.hino_DON import HINO_DON as HamiltonianNet\n",
    "    else:\n",
    "        from networks.hno_DON import *\n",
    "        from networks.hno_DON import HNO_DON as HamiltonianNet\n",
    "elif network == \"modified_deeponet\":\n",
    "    from networks.modified_deeponet import ModifiedDeepONet as OperatorNet\n",
    "    from networks.modified_deeponet import Hparams as OperatorHparams\n",
    "    if use_hino:\n",
    "        from networks.hino_DON import *\n",
    "        from networks.hino_DON import HINO_DON as HamiltonianNet\n",
    "    else:\n",
    "        from networks.hno_DON import *\n",
    "        from networks.hno_DON import HNO_DON as HamiltonianNet\n",
    "elif network == \"fno1d\":\n",
    "    if use_hino:\n",
    "        from networks.hino_DON import *\n",
    "        from networks.hino_DON import HINO_DON as HamiltonianNet\n",
    "    else:\n",
    "        from not_in_use.hno_DON import *\n",
    "elif network == \"fno2d\":\n",
    "    if use_hino:\n",
    "        from networks.hino_DON import *\n",
    "        from networks.hino_DON import HINO_DON as HamiltonianNet\n",
    "    else:\n",
    "        from not_in_use.hno_DON import *\n",
    "elif network == \"fno_timestepping\":\n",
    "    if use_hino:\n",
    "        from networks.hino_DON import *\n",
    "        from networks.hino_DON import HINO_DON as HamiltonianNet\n",
    "    else:\n",
    "        from not_in_use.hno_DON import *\n",
    "else:\n",
    "    raise ValueError(\"Invalid network\")\n",
    "\n",
    "operator_trainer = Trainer.from_checkpoint(checkpoint_path+f\"{network}_{problem}\", \n",
    "                                           OperatorNet, \n",
    "                                           Hparams=OperatorHparams,\n",
    "                                           replicated=replicated,)\n",
    "operator_net = operator_trainer.model\n",
    "operator_net_hparams = operator_trainer.hparams\n",
    "\n",
    "if use_hino:\n",
    "    Trainer.compute_loss = staticmethod(compute_loss_hino)\n",
    "    Trainer.evaluate = eqx.filter_jit(staticmethod(evaluate_hino), donate=\"all-except-first\")\n",
    "else:\n",
    "    Trainer.compute_loss = staticmethod(compute_loss_hno)\n",
    "    Trainer.evaluate = eqx.filter_jit(staticmethod(evaluate_hno), donate=\"all-except-first\")\n",
    "\n",
    "# IMPORT HYPERPARAMETERS\n",
    "with open(hparams_path + \"energy_net\" + '.json', \"rb\") as f:\n",
    "    hparams_energy_net_dict = json.load(f)\n",
    "    energy_net_hparams = EnergyNetHparams(**hparams_energy_net_dict)\n",
    "    \n",
    "energy_net = EnergyNet(energy_net_hparams)\n",
    "\n",
    "model = HamiltonianNet(energy_net=energy_net, operator_net=operator_net)\n",
    "if replicated:\n",
    "    model = eqx.filter_shard(model, replicated)\n",
    "\n",
    "hparams = Hparams(energy_net=energy_net_hparams, operator_net=operator_net_hparams)\n",
    "\n",
    "# INITIALIZE OPTIMIZERS\n",
    "PATIENCE = 5 # Number of epochs with no improvement after which learning rate will be reduced\n",
    "COOLDOWN = 0 # Number of epochs to wait before resuming normal operation after the learning rate reduction\n",
    "FACTOR = 0.5  # Factor by which to reduce the learning rate:\n",
    "RTOL = 1e-4  # Relative tolerance for measuring the new optimum:\n",
    "ACCUMULATION_SIZE = 200 # Number of iterations to accumulate an average value:\n",
    "\n",
    "if network in [\"fno1d\", \"fno2d\", \"fno_timestepping\"]:\n",
    "    θ_optimizer = optax.chain(\n",
    "        conjugate_grads_transform(), # we have to conjugate the gradients for the FNO networks\n",
    "        optax.adam(operator_net_hparams.learning_rate),\n",
    "        reduce_on_plateau(\n",
    "            patience=PATIENCE,\n",
    "            cooldown=COOLDOWN,\n",
    "            factor=FACTOR,\n",
    "            rtol=RTOL,\n",
    "            accumulation_size=ACCUMULATION_SIZE,\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    θ_optimizer = optax.chain(\n",
    "        optax.adam(operator_net_hparams.learning_rate),\n",
    "        reduce_on_plateau(\n",
    "            patience=PATIENCE,\n",
    "            cooldown=COOLDOWN,\n",
    "            factor=FACTOR,\n",
    "            rtol=RTOL,\n",
    "            accumulation_size=ACCUMULATION_SIZE,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "φ_optimizer = optax.chain(\n",
    "    optax.adam(energy_net_hparams.learning_rate),\n",
    "    reduce_on_plateau(\n",
    "        patience=PATIENCE,\n",
    "        cooldown=COOLDOWN,\n",
    "        factor=FACTOR,\n",
    "        rtol=RTOL,\n",
    "        accumulation_size=ACCUMULATION_SIZE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "if operator_net.is_self_adaptive: # Self-adaptive weights are enabled \n",
    "    λ_optimizer = optax.chain(optax.adam(operator_net_hparams.λ_learning_rate), optax.scale(-1.))\n",
    "    opt = optax.multi_transform({'θ': θ_optimizer, 'λ': λ_optimizer, 'φ': φ_optimizer}, param_labels=param_labels_hno_self_adaptive)\n",
    "else:\n",
    "    opt = optax.multi_transform({'θ': θ_optimizer, 'φ': φ_optimizer}, param_labels=param_labels_hno)\n",
    "    \n",
    "opt_state = opt.init(eqx.filter([model], eqx.is_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosjektoppgave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
