{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model: Model(a=weak_c64[])\n",
      "Step 1, model.a: (0.9292898178100586+0.9292898178100586j)\n",
      "Step 2, model.a: (0.8585798740386963+0.8585798740386963j)\n",
      "Step 3, model.a: (0.7878694534301758+0.7878694534301758j)\n",
      "Step 4, model.a: (0.7171594500541687+0.7171594500541687j)\n",
      "Step 5, model.a: (0.6464494466781616+0.6464494466781616j)\n",
      "Step 6, model.a: (0.5757391452789307+0.5757391452789307j)\n",
      "Step 7, model.a: (0.505029022693634+0.505029022693634j)\n",
      "Step 8, model.a: (0.434318870306015+0.434318870306015j)\n",
      "Step 9, model.a: (0.36360877752304077+0.36360877752304077j)\n",
      "Step 10, model.a: (0.29289859533309937+0.29289859533309937j)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "class Model(eqx.Module):\n",
    "    a: jax.Array  # Using JAX arrays to handle complex numbers\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = jnp.array(1.0 + 1.0j)\n",
    "        \n",
    "    def __call__(self):\n",
    "        return jnp.imag(self.a) + jnp.real(self.a)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "def loss_fn(model):\n",
    "    return model()\n",
    "\n",
    "print(\"Initial model:\", model)\n",
    "\n",
    "# Custom transformation to conjugate gradients\n",
    "def conjugate_grads_transform():\n",
    "    def init_fn(params):\n",
    "        # Returns an empty state\n",
    "        return None\n",
    "\n",
    "    def update_fn(updates, state, params=None):\n",
    "        # Conjugate the gradients if they are complex\n",
    "        updates = jax.tree_util.tree_map(\n",
    "            lambda g: jnp.conj(g) if jnp.iscomplexobj(g) else g, updates\n",
    "        )\n",
    "        return updates, state\n",
    "\n",
    "    return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "# Combine conjugate transformation with Adam optimizer\n",
    "opt = optax.chain(\n",
    "    conjugate_grads_transform(),  # Conjugate gradients\n",
    "    optax.adam(0.1)             # Adam optimizer\n",
    ")\n",
    "opt_state = opt.init(model)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    grad = eqx.filter_grad(loss_fn)(model)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    print(f\"Step {i + 1}, model.a: {model.a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model: Model(a=weak_f32[], b=weak_f32[])\n",
      "Step 1, model.a: 0.9000006914138794, model.b: 0.9000006914138794\n",
      "Step 2, model.a: 0.8000016808509827, model.b: 0.8000016808509827\n",
      "Step 3, model.a: 0.7000020742416382, model.b: 0.7000020742416382\n",
      "Step 4, model.a: 0.6000030636787415, model.b: 0.6000030636787415\n",
      "Step 5, model.a: 0.5000039935112, model.b: 0.5000039935112\n",
      "Step 6, model.a: 0.400004506111145, model.b: 0.400004506111145\n",
      "Step 7, model.a: 0.3000052869319916, model.b: 0.3000052869319916\n",
      "Step 8, model.a: 0.20000602304935455, model.b: 0.20000602304935455\n",
      "Step 9, model.a: 0.1000068262219429, model.b: 0.1000068262219429\n",
      "Step 10, model.a: 7.495284080505371e-06, model.b: 7.495284080505371e-06\n"
     ]
    }
   ],
   "source": [
    "class Model(eqx.Module):\n",
    "    a: jax.Array  # Using JAX arrays to handle complex numbers\n",
    "    b: jax.Array\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = jnp.array(1.0)\n",
    "        self.b = jnp.array(1.0)\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.a + self.b\n",
    "\n",
    "model = Model()\n",
    "\n",
    "def loss_fn(model):\n",
    "    return model()\n",
    "\n",
    "print(\"Initial model:\", model)\n",
    "\n",
    "# Custom transformation to conjugate gradients\n",
    "def conjugate_grads_transform():\n",
    "    def init_fn(params):\n",
    "        # Returns an empty state\n",
    "        return None\n",
    "\n",
    "    def update_fn(updates, state, params=None):\n",
    "        # Conjugate the gradients if they are complex\n",
    "        updates = jax.tree_util.tree_map(\n",
    "            lambda g: jnp.conj(g) if jnp.iscomplexobj(g) else g, updates\n",
    "        )\n",
    "        return updates, state\n",
    "\n",
    "    return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "# Combine conjugate transformation with Adam optimizer\n",
    "opt = optax.adam(0.1)\n",
    "opt_state = opt.init(model)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    grad = eqx.filter_grad(loss_fn)(model)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    print(f\"Step {i + 1}, model.a: {model.a}, model.b: {model.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 100\n",
    "out_channels = 150\n",
    "\n",
    "scale = 1.0 / (in_channels * out_channels)\n",
    "v = random.uniform(random.key(0),(in_channels, 40, 50),minval=-scale, maxval=+scale)\n",
    "bypass = random.uniform(random.key(0),(out_channels, in_channels),minval=-scale, maxval=+scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = jnp.tensordot(bypass, v, axes=(1, 0))\n",
    "result2 = jnp.einsum(\"oi,ixy->oxy\", bypass, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 ms ± 41.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.block_until_ready(jnp.einsum(\"oi,ixy->oxy\", bypass, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosjektoppgave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
