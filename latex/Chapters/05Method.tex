% !TeX root = ..\main.tex
\chapter{Method}

\section{Data generation}
The data used to train the different networks has been generated, as we are trying to compare the models in a systematic way on familiar problems.
The data structure is consistent across all different problems and models, and consists of input-output function pairs \(\{a_i, u_i\}_{i=1}^N\).

We generated \(N = 1000\) samples, divided into:
\begin{itemize}
    \item \textbf{Training set:} 800 samples
    \item \textbf{Validation set:} 100 samples
    \item \textbf{Test set:} 100 samples
\end{itemize}

The data was generated on equispaced grids:
\begin{align*}
    x_m &= m \Delta x \in [0, 20), &&m = 0, \dots, 256-1\\
    t_n &= n \Delta t \in [0, 3), &&n = 0, \dots, 384-1,
\end{align*}

i.e. with step sizes \(\Delta x = \frac{20}{256} \approx 0.078\) and \(\Delta t = \frac{3}{384} \approx 0.0078\). For the advection problem, we may use an analytical solution, but for the Korteweg–De Vries equation, we need to solve the equation numerically.
The step sizes are therefore chosen to be small enough to avoid numerical instability as well, to make sure that our traning data follows the governing PDE.

The number of grid points was meticulously chosen such that we can downsample to a \(64 \times 64\) grid, which we will be used as training and validation data:
\begin{align*}
    x_{\hat{m}} &= \hat{m}\cdot 4 \Delta x \in [0, 20), &&\hat{m} = 0, \dots, 64-1\\
    t_{\hat{n}} &= \hat{n} \cdot 4 \Delta t \in [0, 2), &&\hat{n} = 0, \dots, 64-1,
\end{align*}

In this way, we can explore how the different models are able to interpolate when encountering data on a finer grid, and 
extrapolate to time points further into the future.
The \(64 \times 64\)-grid was chosen specifically, as \(64\) is a power of \(2\), which better utilizes the Fast Fourier Transform seen in the Fourier Neural Operators.

To test for extrapolation, we will use the prediction at \(t\approx 2\) as the initial condition for the next time steps.
This is in particular crucial for the \texttt{FNO2d}, which assumes a periodic domain, and simply giving a larger time domain will then lead to wrong predictions for all time steps.
We also tried simply using larger values of \(t\) to extrapolate for the other methods, but this gave very poor results for all architectures.

\subsection{Preprocessing and weight initialization}\label{sec:scaling}

Normalizing the input and output data has been extensively used in deep learning, as it may aid in faster convergence \sidecite{huangNormalizationTechniquesTraining2020}.
However, as \sidecite{wangImprovedArchitecturesTraining2022a} points out, it is somewhat unclear how to best preprocess the 
data when working with operator networks, and if this should differ from the regular normalization techniques, as it has not been extensively studied yet.

In \sidecite{xuPreprocessingPhysicsinformedNeural2024}, the authors show how preprocessing, specifically Z-score normalization, can improve the performance of PINNs.
With that in mind, we also choose to normalize the data to have zero mean and unit variance.
We then have the new inputs and outputs given by
\begin{equation}
    \hat{u} = \frac{u - \mu_u}{\sigma_u}, \quad \hat{x} = \frac{x - \mu_x}{\sigma_x}, \quad \hat{t} = \frac{t - \mu_t}{\sigma_t},
    \label{eq:preprocessing}
\end{equation}
and the normalized initial condition is given by \(\hat{a} = \hat{u}|_{t=0}\).

As noted in \cite{xuPreprocessingPhysicsinformedNeural2024}, it is now important to also scale the equations we want to impose accordingly.
For instance, we will have to use \(u_t = \hat{u}_{\hat{t}} \frac{\sigma_u}{\sigma_t}, u_x = \hat{u}_{\hat{x}} \frac{\sigma_u}{\sigma_x}\), which follows from the chain rule.
For the energy density net, we will however use the input in its original scale. The reason for this is that the spatial derivatives in particular become very large, due to the small scale of the spatial domain.
This leads to large predictions, far away from the actual energy density, which is not ideal.

When we compute metrics on the test data and show predictions, we will however use the actual grids and decode the predictions 
back to their original scale. A further exploration of different ways of preprocessing the data could give new insights and uncover improved techniques for 
training neural networks on function spaces, but has not been explored further in this project.

Weight initialization has also not been extensively studied in the context of operator learning. For traditional neural networks, it is common to initialize the weights with either "He" or "Xavier" initializations \sidecite{glorotUnderstandingDifficultyTraining,heDelvingDeepRectifiers2015}.
In this project, we have used the gelu activation function, and have therefore chosen to use the "He" initialization, as it is specifically designed for ReLU-like activation functions. 

\section{Test problems}
We have chosen to work with two different test problems: the advection equation and the Korteweg–De Vries equation. The prior is a simple linear PDE, and acts as a good starting point for testing the models, while the latter is a nonlinear PDE, and is therefore a more challenging and more realistic problem.

\subsection{Advection equation}
The governing equation for the advection problem is given by
\begin{equation}
    u_t + c u_x = 0.
    \label{eq:advection}
\end{equation}
That is, a translation of the initial condition at speed \(c\).
Here, we have chosen to use a one-soliton as the initial condition and periodic boundary, with analytical solution given by

\begin{equation}
    \label{eq:advection_solution}
    u(x,t) = 2 \, c \,\text{sech}^2\left(\left(x - c t + \frac{P}{2} + d\, P\right) \mod P - \frac{P}{2}\right)
\end{equation}

where \(c \sim \mathcal{U}(0.5, 1.5)\) and \(d \sim \mathcal{U}(0, 1)\) are random variables, and \(P=20\) is the periodicity of the domain.

To relate this to the form of \cref{eq:hamiltonian_pde}, we note that the energy is given by the Hamiltonian

\begin{equation}
    \mathcal{H}[u] = \int_\mathbb{R} \frac{c}{2} u^2 \, dx.
    \label{eq:advection_energy}
\end{equation}

with variatonal derivative

\begin{equation}
    \frac{\delta \mathcal{H}[u]}{\delta u} = c u,
    \label{eq:advection_variational_derivative}
\end{equation}

and we have \(\mathcal{G} = -\partial_x\).

\subsection{Korteweg–De Vries equation}
The Korteweg–De Vries equation is given by

\begin{equation}
    u_t + \eta u u_x + \gamma^2 u_{xxx} = 0 \qquad x \in [0,20], \quad t > 0
    \label{eq:kdv}
\end{equation}

where we have set \(\eta = 6\) and \(\gamma = 1\). We have also truncated the domain to \([0,20]\), and use periodic boundary conditions.

Again, we reformulate \cref{eq:kdv} to the form of \cref{eq:hamiltonian_pde}. Here, we have \(\mathcal{G}=-\partial_x\), and the energy is given by

\begin{equation}
    \mathcal{H}[u] = \int_\mathbb{R} \left(\frac{\eta}{6} u^3 - \frac{\gamma^2}{2}u_x^2 \right)\, dx
    \label{eq:kdv_energy}
\end{equation}

with varitional derivative

\begin{equation}
    \frac{\delta\mathcal{H}[u]}{\delta u} = \frac{\eta}{2} u^2 - \gamma^2u_{xx}.
    \label{eq:kdv_variational_derivative}
\end{equation}

This gives us the Hamiltonian formulation

\begin{equation}
    u_t = -\left(\frac{\eta}{2} u^2 + \gamma^2 u_{xx}\right)_x.
    \label{eq:kdv_hamiltonian_pde}
\end{equation}

For the initial condition, we have used a two-soliton, given by
\begin{equation}
    \label{eq:kdv_initial_condition}
        a(x) = 2 \sum_{l=1}^2 c_l^2 \, \text{sech}^2\left(c_l \, \left(\left(x+\frac{P}{2}-d_l \, P\right) \mod P - \frac{P}{2}\right)\right),
\end{equation}

where \(c_1, c_2 \sim \mathcal{U}(0.5, 1.5)\) and \(d_1, d_2 \sim \mathcal{U}(0, 1)\) are random variables, and \(P=20\) is the period.

With a two-soliton as the initial condition, it is however no longer possible to find an analytic solution to the KdV equation, and we therefore need to solve it numerically.
As stated in \sidecite{leimkuhlerSimulatingHamiltonianDynamics2005}, we need to ensure that we use a skew-symmetric approximation of the \(\mathcal{G}\)-operator in \cref{eq:hamiltonian_pde}. Since we have \(\mathcal{G}=-\partial_x\), we can approximate it by central differences.
In time, we should use a symplectic integrator, so that we can conserve an approximation of the energy of the system. This is important, as we will later be training models which will attempt to conserve the energy of the system.

To avoid inaccurate solutions, we use a fine grid and methods of high order. We use a sixth-order Gauss-Legendre method for time integration and sixth-order central differences for spatial discretization, to ensure that the data is accurate to the system we want to learn. During each implicit time step, the Newton method is employed with settings \texttt{atol} = $1 \times 10^{-12}$, \texttt{rtol} = $1 \times 10^{-12}$, and \texttt{maxiter} = 10.

\section{Hyperparameter optimization}

A challenging task in training neural networks, is to pick the best possible hyperparameters. The hyperparameters are the parameters that are not learned during the training process, but rather have to be set in advance by the user.
Choosing the wrong parameters could potentially lead to a model that performs drastically worse than it would do with a better set of hyperparameters. To fairly compare different methods, it is therefore essential to try to find the most optimal hyperparameters for each model and problem.

In this project, we will utilize the package \texttt{optuna} to optimize the hyperparameters, which is an automatic hyperparameter optimization framework, in combination with some manual tuning. 
It should however also be noted that finding good hyperparameters is a computationally expensive task, and we have therefore not been able to explore the full hyperparameter space for each model.
In addition, the best hyperparameters will in general also depend on the specific problem, and we have therefore chosen to optimize the hyperparameters for each problem separately.
The search spaces for the different models are given in the following tables, with the hyperparameters we ended up using.

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the DeepONet model}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{DeepONet Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter & Range & Value for advection & Value for KdV \\
    \midrule
    trunk width & [50, 150] & 85 & 66 \\
    branch width & [50, 150] & 126 & 57\\
    trunk depth & [3, 10] & 6 & 8 \\
    branch depth & [3, 10] & 7 & 9\\
    interact size & [5, 25] & 25 & 24 \\
    learning rate  & [1e-4, 1e-1] & 1.6e-4 & 2.9e-3\\
    number of sensors & \{16,32,64\} & 64 & 64\\
    number of query points & [50, 2000] & 1568 & 722\\ 
    self-adaptive learning rate & [1e-4, 1e-1] & 6.7e-2 & 3.0e-2\\
    mask & - & logistic, sharp & polynomial, sharp \\
    \label{tab:hyperparameter_search_space_don}
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the Modified DeepONet model}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{Modified DeepONet Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter & Range & Value for advection & Value for KdV \\
    \midrule
    width & [50, 150] & 150 & 125 \\
    depth & [3, 10] & 3 & 5 \\
    interact size & [5, 25] & 6 & 10 \\
    learning rate  & [1e-4, 1e-1] & 2.4e-3 & 4.6e-3\\
    number of sensors & \{16,32,64\} & 32 & 32\\
    number of query points & [50, 2000] & 1673 & 1896\\ 
    self-adaptive learning rate & [1e-4, 1e-1] & 6.8e-3 & 4.1e-2\\
    mask & - & exponential, smooth & polynomial, learnable \\
    \label{tab:hyperparameter_search_space_mod_don}
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the FNOTimeStepping model}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{FNOTimeStepping Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter & Range & Value for advection & Value for KdV \\
    \midrule
    fourier layers & [3, 6] & 4 & 5 \\
    hidden dimension & [50, 150] & 77 & 77 \\
    max number of modes & [8, 32] & 14 & 26 \\
    \(\Delta t\)  & - & \(\frac{1}{8}\) & \(\frac{1}{8}\)\\
    learning rate & [1e-4, 1e-1] & 8.6e-4 & 3.7e-3\\
    self-adaptive learning rate & [1e-4, 1e-1] & - & 8.2e-2\\
    mask & - & - & exponential, smooth \\
    \label{tab:hyperparameter_search_space_fno_timestepping}
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the FNO1d model}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{FNO1d Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter & Range & Value for advection & Value for KdV \\
    \midrule
    fourier layers & [3, 6] & 6 & 6 \\
    hidden dimension & [50, 150] & 110 & 110 \\
    max number of modes & [8, 32] & 27 & 27 \\
    learning rate & [1e-4, 1e-1] & 6.8e-4 & 6.8e-4\\
    self-adaptive learning rate & [1e-4, 1e-1] & 6.8e-3 & 3.8e-2\\
    mask & - & exponential, smooth & exponential, smooth \\
    \label{tab:hyperparameter_search_space_fno1d}
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the FNO2d model}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{FNO2d Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter & Range & Value for advection & Value for KdV \\
    \midrule
    fourier layers & [3, 6] & 5 & 4 \\
    hidden dimension & [50, 150] & 110 & 90 \\
    max number of modes & [8, 32] & 27 & 8 \\
    learning rate & [1e-4, 1e-1] & 6.8e-4 & 1.2e-2\\
    self-adaptive learning rate & [1e-4, 1e-1] & 1.2e-2 & -\\
    mask & - & logistic, smooth & - \\
    \label{tab:hyperparameter_search_space_fno2d}
    \end{tabular}
\end{table}

We did not perform a hyperparameter search for the energy net, but ended up using the parameters given in \cref{tab:hyperparameter_search_space_energy_net} for both problems.

\begin{table}[h]
    \centering
    \caption{Hyperparameters for the energy net}
    \begin{tabular}{@{}llll@{}}
    \multicolumn{4}{c}{\textbf{Energy net Hyperparameters}} \\  % Title for the first set
    \cmidrule(r){1-4}  % Partial horizontal line
    Hyperparameter &  Value \\
    \midrule
    width & 50 \\
    depth & 3 \\
    learning rate & 1e-3 \\
    \label{tab:hyperparameter_search_space_energy_net}
    \end{tabular}
\end{table}

Furthermore, we have for all problems used the Adam optimizer with weight decay, a batch size of 16 and gelu as the activation function.
We also used the learning rate scheduler \texttt{optax.contrib.reduce\_on\_plateau}, which halves the step size when the training loss plateaus.

\section{Loss function}

The choice of loss function plays an important role in training any neural network. Since we are working with mappings between function spaces, we should define a loss function that is suitable for this task.
Inspired by \sidecite{kovachkiNeuralOperatorLearning2024}, we want to minimize the \(L_\mu^2(\mathcal{A}, \mathcal{U})\) Bochner norm of the difference between the true operator \(\mathcal{S}\) and the learned operator \(\mathcal{S}_\theta\), that is 


\begin{equation}
    \|\mathcal{S}-\mathcal{S}_\theta\|_{L_\mu^2(\mathcal{A}, \mathcal{U})}^2 = \mathbb{E}_{a\sim\mu}\|\mathcal{S}(a)-\mathcal{S}_\theta(a)\|_{L^2(\mathcal{Y})}^2. 
\end{equation}

It has been observed in \sidecite{wangImprovedArchitecturesTraining2022a, leoniDeepONetPredictionLinear2021} that DeepONets might be biased towards learning functions with larger magnitudes, and they therefore argue that a relative loss function might be better suited.
In \cite{kovachkiNeuralOperatorLearning2024}, they also use a relative loss function due to its "good normalization and regularization effect". We therefore investigate the relative \(L_\mu^2(\mathcal{A}, \mathcal{U})\) error, as stated in \cite{kovachkiNeuralOperatorLearning2024}:


\begin{equation}
    \mathbb{E}_{a\sim\mu}\frac{\|\mathcal{S}[a]-\mathcal{S}_\theta[a]\|_{L^2(\mathcal{Y})}}{\|\mathcal{S}[a]\|_{L^2(\mathcal{Y})}}.
    \label{eq:expected_relative_error}
\end{equation}

\begin{proposition}
    \hfill\\
    The expected relative \(L_2\) error can be approximated by the relative \(\ell_2\) error of a mini-batch:

    \begin{equation}
        \mathbb{E} \frac{1}{N} \sum_{i=1}^{N} \frac{\|\vect{\mathcal{S}[a_i]}- \vect{\mathcal{S}_\theta[a_i]}\|_2}{\|\vect{\mathcal{S}[a_i]}\|_2}=\mathbb{E}_{a\sim\mu}\frac{\|\mathcal{S}[a]-\mathcal{S}_\theta[a]\|_{L^2(\mathcal{Y})}}{\|\mathcal{S}[a]\|_{L^2(\mathcal{Y})}}.
    \end{equation}

    where \(N\) is the batch size. 
    \(\vect{\mathcal{S}[a_i]}=[\mathcal{S}[a_i](y_1), \ldots, \mathcal{S}[a_i](y_P)]^\top\) 
    is a vector of the true values at \(P\) points; either over the whole grid, or \(P\) uniformly sampled query points in the case of the DeepONet. 
    The same also holds for \(\vect{\mathcal{S}_\theta[a_i]}=[\mathcal{S}_\theta[a_i](y_1), \ldots, \mathcal{S}_\theta[a_i](y_P)]^\top\), but for the predicted values.
\end{proposition}

\begin{proof}
    To approximate the expectation value, we use a standard Monte Carlo estimator, i.e. the sample mean of a mini-batch:

    \begin{equation}
        \mathbb{E}_{MC}\frac{\|\mathcal{S}[a]-\mathcal{S}_\theta[a]\|_{L^2(\mathcal{Y})}}{\|\mathcal{S}[a]\|_{L^2(\mathcal{Y})}} = \frac{1}{N} \sum_{i=1}^{N} \frac{\|\mathcal{S}[a_i]-\mathcal{S}_\theta[a_i]\|_{L^2(\mathcal{Y})}}{\|\mathcal{S}[a_i]\|_{L^2(\mathcal{Y})}}, 
        \qquad a_i \overset{\text{i.i.d.}}{\sim} \mu.
    \end{equation}

    To approximate the \(L_2\) norms, we may use a standard Monte Carlo integral:

    \begin{equation}
        \|f(y)\|_{L^2(\mathcal{Y})}^2 = \int_{\mathcal{Y}} f(y)^2 dy \approx \text{Vol}(\mathcal{Y}) \, \frac{1}{P} \sum_{j=1}^{P} f(y_j)^2
    \end{equation}

    \Cref{eq:expected_relative_error} can then be approximated by
    
    \begin{equation}
        \frac{1}{N} \sum_{i=1}^{N} \frac{\sqrt{\frac{1}{P} \sum_{j=1}^{P}  (\mathcal{S}[a_i](y_j)-\mathcal{S}_\theta[a_i](y_j))^2}}{\sqrt{\frac{1}{P} \sum_{j=1}^{P} (\mathcal{S}[a_i](y_j))^2}}
        = \frac{1}{N} \sum_{i=1}^{N} \frac{\|\vect{\mathcal{S}[a_i]}- \vect{\mathcal{S}_\theta[a_i]}\|_2}{\|\vect{\mathcal{S}[a_i]}\|_2}
    \end{equation}

which is a consistent estimator by the law of large numbers.
\end{proof}

For our operator loss, we will therefore use

\begin{equation}
    \mathcal{L}_{\text{operator}}(\theta)=\frac{1}{N} \sum_{i=1}^{N} \frac{\|\vect{\mathcal{S}[a_i]}- \vect{\mathcal{S}_\theta[a_i]}\|_2}{\|\vect{\mathcal{S}[a_i]}\|_2}.
    \label{loss}
\end{equation}

To evaluate the validation/test error, we will use the same approximation to \ref{eq:expected_relative_error} as above. In the case of the DeepONets, where we randomly sample query points during the training, 
we will however use all of the grid points during inference. This is to ensure that we get a fair comparison between the different models, and an accurate estimate of the validation/test error.

We also found that some of the models did not predict good results for \(t=0\), which should be the identity operator. 
Therefore, we added a penalty term to penalize violations of the initial condition, given by

\begin{equation}
    \mathcal{L}_{\text{init}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \frac{\|\vect{a_i}- \vect{\mathcal{S}_\theta[a_i]}|_{t=0}\|_2}{\|\vect{a_i}\|_2}.
\end{equation}

For the vanilla operator networks, we have then used the following loss function:

\begin{equation}
    \mathcal{L}(\theta)=\mathcal{L}_{\text{operator}}(\theta)+ \lambda_{\text{init}} \mathcal{L}_{\text{init}}(\theta)
    \label{loss_with_init_penalty}
\end{equation}

where we have set the penalty weight \(\lambda_{\text{init}} = 0.5\).

When it comes to the Hamiltonian Operator Networks, we have added an additional energy penalty term to the loss function, as suggested in \sidecite{tanakaNeuralOperatorsMeet2024}.
The energy penalty term is given by

\begin{equation}
    \mathcal{L}_{\text{energy}}(\phi)=\frac{1}{N} \sum_{i=1}^{N} \left\|\dot{\vect{u}}_i^\theta - \vect{\mathcal{G} \frac{\delta \mathcal{H}_\phi[u_i^\theta]}{\delta u_i^\theta}}\right\|_2.
    \label{energy_penalty}
\end{equation}

Since we do not have access to the true temporal derivative, we have not used a relative error for this term.
We add this to the operator loss function, to get the final loss function for the Hamiltonian Operator Networks:

\begin{equation}
    \mathcal{L}(\theta, \phi)=\mathcal{L}_{\text{operator}}(\theta)+ \lambda_{\text{init}} \mathcal{L}_{\text{init}}(\theta) + \lambda_{\text{energy}} \mathcal{L}_{\text{energy}}(\phi)
    \label{loss_with_init_penalty_hon}
\end{equation}

where we have multiplied with the penalty weight \(\lambda_{\text{energy}} = 1e-3\).

\section{Self-adaptive weights}

In addition to the weights of the neural network, which we denote by \(\theta\), we also supply 
our operator networks with the possibility for learning self-adaptive weights, denoted by \(\lambda\).
The self-adaptive weights were first introduced in \sidecite{mcclennySelfAdaptivePhysicsInformedNeural2023a}, as a 
way to improve the learning of PINNs by adaptively weighting the different terms and points in the loss function. 
Instead of directly minimizing the loss function, we first maximize it with respect to the self-adaptive weigths. 
The idea behind this, is that it should force the network to focus on the most challenging terms and points in the domain to learn \cite{mcclennySelfAdaptivePhysicsInformedNeural2023a}.

For our problem setting, it makes sense to adaptively weight the different time points, as it is likely easier 
to learn the solution operator to earlier time points. Since we for all methods are working with training sets with \(64\) time points, 
we can define the self-adaptive weights as \(\vect{\lambda} = [\lambda_1, \ldots, \lambda_{64}]\), where \(\lambda_i\) is the weight for the \(i\)-th time point.
Our relative loss function is then modified to the following:

\begin{equation}
    \mathcal{L}_{\text{operator}}(\theta, \lambda) = 
    \frac{1}{N} \sum_{i=1}^{N} 
    \frac{\sqrt{\sum_{j=1}^P g(\lambda_{j*}) 
    (\mathcal{S}[a_i](y_j)-\mathcal{S}_\theta[a_i](y_j))^2}}
    {\|\vect{\mathcal{S}[a_i]}\|_2},
\end{equation}

and in each update we compute the gradients according to 

\begin{equation}
    \min_{\theta} \max_{\lambda}
    \mathcal{L}(\theta, \lambda). 
\end{equation}

\(g(\lambda)\) is a "masking function", and \(\lambda_{j*}\) corresponds to the temporal coordinate of the point \(y_j\). 
It is then multiplied with the squared difference between the true and predicted function values.
We note that when \(g(\lambda) = 1\), the loss function is equivalent to the un-weighted loss presented before, \cref{loss}.
These weights are furthermore only used during training, and discarded during inference.

As explained in \sidecite{mcclennySelfAdaptivePhysicsInformedNeural2023a}, 
we want the masking function defined on \([0, \infty)\) to be 
non-negative, differentiable on \((0, \infty)\), 
and strictly increasing. We can compute the gradient of the loss function with respect to the self-adaptive weights as follows:

\begin{equation}
    (\nabla_{\lambda} \mathcal{L}_{\text{operator}})_{j*} = \frac{1}{2 N} \sum_{i=1}^{N}
    \frac{1}{\|\vect{\mathcal{S}[a_i]}\|_2} 
    \frac{\sum_{j=1}^P g'(\lambda_{j*}) 
    \left(\mathcal{S}[a_i](y_j)-\mathcal{S}_\theta[a_i](y_j)\right)^2}
    {\sqrt{\sum_{j=1}^P g(\lambda_{j*}) 
    \left(\mathcal{S}[a_i](y_j)-\mathcal{S}_\theta[a_i](y_j)\right)^2}}.
\end{equation}

We then know that \((\nabla_{\lambda} \mathcal{L})_{j*}\geq 0\), with equality if and only if the (original) un-weighted loss is zero, as we have \(g'(\lambda) > 0\). 
The updates should then also be larger where the un-weighted loss is larger, which is the desired behavior.

As candidates for the masking function, we have considered the three following functions:

\begin{multicols}{3}
\centering
\textbf{Exponential Mask} \\[0.5em]
$g(\lambda) = e^{a(\lambda - 1)}$ 
\columnbreak

\centering
\textbf{Polynomial Mask} \\[0.5em]
$g(\lambda) = 
\begin{cases} 
\lambda^{a}, & \lambda \geq 1, \\ 
e^{a(\lambda - 1)}, & \lambda < 1 
\end{cases}$
\columnbreak

\centering
\textbf{Logistic Mask} \\[0.5em]
$g(\lambda) = \frac{2}{1 + e^{a (1-\lambda)}}$.
\end{multicols}

Here, the parameter \(a\geq0\) decides the "sharpness" of the function. This is either chosen to be a hyperparameter that we
choose before training, or we can let it be a learnable parameter. For the fixed masking functions, we have chosen
\(a \in \{1,3\}\) for the exponential and polynomial masks, and \(a \in \{5, 50\}\) for the logistic mask. The fixed masks are
illustrated in \cref{fig:masking_functions}.

\begin{figure}[h!]
    \centering
    \includesvg{masks}
    \caption[Masking functions]{Comparison of the different available masking functions.}
    \label{fig:masking_functions}
\end{figure}

Of these, versions similar to our logistic and polynomial masks were 
mentioned in \sidecite{mcclennySelfAdaptivePhysicsInformedNeural2023a}, but we also
allow for learnable masks. We will then let \texttt{optuna} decide between these six fixed masks or the learnable versions in the hyperparameter optimization step.

In \cite{mcclennySelfAdaptivePhysicsInformedNeural2023a} it is also mentioned that we should bound the weights.
Otherwise, we run the risk of increasing the loss drastically for masks unbounded from above, or we may encounter vanishing gradients in the case of logistic masks.
We therefore propose centering the self-adaptive weights around 1, by updating the weights according to

\begin{equation}
    \lambda \leftarrow g^{-1}\left(\frac{\lambda}{\frac{1}{64} \sum_{j=1}^{64} \lambda_j}\right)
\end{equation}

after each step. We then have 
\[
\frac{1}{64}\sum_{j=1}^{64} g\left(\lambda_j^{(\text{new})}\right) 
=
\frac{1}{64}\sum_{j=1}^{64} g
\left(g^{-1}
\left(\frac{\lambda_j^{(\text{old})}}{\frac{1}{64} \sum_{j=1}^{64} \lambda_j^{(\text{old})}}
\right)\right)
=
\frac{\frac{1}{64} \sum_{j=1}^{64} \lambda_j^{(\text{old})}}{\frac{1}{64} \sum_{j=1}^{64} \lambda_j^{(\text{old})}}
=
1.
\]
We also clip the input to the inverse masking function, to avoid logarithms of negative numbers.

\section{Periodic boundary conditions}

We have chosen to work with test problems that have periodic boundary conditions, which we want the solutions of our models to respect.
To ensure this, we can choose between hard and soft constraints \sidecite{karniadakisPhysicsinformedMachineLearning2021}. Soft constraints are easier and more general 
to implement, as this involves adding a penalty term to the loss function. We have chosen to work with hard constraints,
as this will lead to the boundary conditions being satisfied exactly.

For the DeepONet, we follow the suggestion of \sidecite{luComprehensiveFairComparison2022}. Their solution is to encode the spatial 
part of our query point in a Fourier basis, i.e. \(x \to \left[\sin(\frac{2\pi x}{P}), \cos(\frac{2\pi x}{P})\right]\), where \(P\) is the periodicity.
The periodicity in \(x\) of the model then clearly follows from the periodicity in \(\sin\) and \(\cos\).

For the FNOs, we note that both the discrete Fourier and the inverse discrete Fourier transform are \(N\)-periodic (with \(N\) being the number of
elements in the sequence to transform). 
The output from the kernel integral operator, \(\mathcal{K}\), will therefore strictly obey the periodic boundary conditions, no matter if the input function is or not.
On the other hand, the local transform operator, \(\mathcal{W}\), allow for outputs that are not periodic.
In fact, since the action is pointwise, the output will observe the same periodicity as the input.
Since we append the grid to the initial input function (the initial condition \(a\)), we can also perform a Fourier feature expansion of the spatial grid, in order to preserve the spatial periodicity.
The bias functions, \(b\), are not periodic in general, but we can enforce this directly. Here are three different approaches:

\begin{itemize}
    \item \textbf{Single vector:} We can parametrize the bias function as a single vector, i.e. a 
    constant function. The bias term is then clearly periodic in both \(x\) and \(t\).\\
    \item \textbf{Fourier series:} We write our bias function as a truncated Fourier series with learnable coefficients:\\
    \[\begin{aligned}
    b(x,t) &= \sum_{n=0}^{N_{max}}\sum_{m=0}^{M_{max}}\alpha_{m,n} \cos{\frac{2\pi m x}{P_x}}\cos{\frac{2\pi n t}{P_t}}\\
    &+ \sum_{n=0}^{N_{max}}\sum_{m=0}^{M_{max}} \beta_{m,n} \cos{\frac{2\pi m x}{P_x}}\sin{\frac{2\pi n t}{P_t}}\\
    &+ \sum_{n=0}^{N_{max}}\sum_{m=0}^{M_{max}} \gamma_{m,n} \sin{\frac{2\pi m x}{P_x}}\cos{\frac{2\pi n t}{P_t}}\\
    &+ \sum_{n=0}^{N_{max}}\sum_{m=0}^{M_{max}} \delta_{m,n} \sin{\frac{2\pi m x}{P_x}}\sin{\frac{2\pi n t}{P_t}}
    \end{aligned}\]
    To avoid periodicity in \(t\), we can make sure that the "period" is larger than the length of the domain, 
    i.e. \(P_t > T\).\\
    \item \textbf{Fourier basis:} Similarly to the trunk net in the DeepONet, we can encode the spatial part of the query point in a Fourier basis, and take
    \(\left[\sin(\frac{2\pi x}{P}), \cos(\frac{2\pi x}{P}), t\right]\) as input to \(b\). The bias function can then be parametrized as a (shallow) neural network, 
    or we can for instance use a polynomial basis.
\end{itemize}

In the section on Hamiltonian (informed) neural operators, we discussed the need to remove the local linear transform \(\mathcal{W}_L\) of the last layer, in order to be able to compute exact derivatives. 
To avoid outputs that are periodic in time, we therefore need to add a bias function that is periodic in space, but not in time.
In this project, we have decided to use the Fourier series approach explained above.

\section{Implementation}

A very large portion of this project has been spent on the efficient implementation of the different models.
The code for this project is written in \texttt{Python}, and utilizes the \texttt{JAX} library. 
There are many deep learning libraries one can use in conjunction with \texttt{JAX}, and in this project we have opted for the \texttt{Equinox} package \sidecite{kidgerEquinoxNeuralNetworks2021}.