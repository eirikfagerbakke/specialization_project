@online{caoLNOLaplaceNeural2023,
  title = {{{LNO}}: {{Laplace Neural Operator}} for {{Solving Differential Equations}}},
  shorttitle = {{{LNO}}},
  author = {Cao, Qianying and Goswami, Somdatta and Karniadakis, George Em},
  date = {2023-05-30},
  eprint = {2303.10528},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.10528},
  url = {http://arxiv.org/abs/2303.10528},
  urldate = {2025-01-03},
  abstract = {We introduce the Laplace neural operator (LNO), which leverages the Laplace transform to decompose the input space. Unlike the Fourier Neural Operator (FNO), LNO can handle non-periodic signals, account for transient responses, and exhibit exponential convergence. LNO incorporates the pole-residue relationship between the input and the output space, enabling greater interpretability and improved generalization ability. Herein, we demonstrate the superior approximation accuracy of a single Laplace layer in LNO over four Fourier modules in FNO in approximating the solutions of three ODEs (Duffing oscillator, driven gravity pendulum, and Lorenz system) and three PDEs (Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system). Notably, LNO outperforms FNO in capturing transient responses in undamped scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's exact representation of the pole-residue formulation yields significantly better results than FNO. For the nonlinear reaction-diffusion system, LNO's errors are smaller than those of FNO, demonstrating the effectiveness of using system poles and residues as network parameters for operator learning. Overall, our results suggest that LNO represents a promising new approach for learning neural operators that map functions between infinite-dimensional spaces.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\869S4CI2\\Cao et al. - 2023 - LNO Laplace Neural Operator for Solving Differential Equations.pdf;C\:\\Users\\eirik\\Zotero\\storage\\QU5RTDN3\\2303.html}
}

@article{celledoniPreservingEnergyResp2012,
  title = {Preserving Energy Resp. Dissipation in Numerical {{PDEs}} Using the “{{Average Vector Field}}” Method},
  author = {Celledoni, E. and Grimm, V. and McLachlan, R.I. and McLaren, D.I. and O’Neale, D. and Owren, B. and Quispel, G.R.W.},
  date = {2012-08},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {231},
  number = {20},
  pages = {6770--6789},
  issn = {00219991},
  doi = {10.1016/j.jcp.2012.06.022},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999112003373},
  urldate = {2025-01-13},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\QBNM7L7C\Celledoni et al. - 2012 - Preserving energy resp. dissipation in numerical PDEs using the “Average Vector Field” method.pdf}
}

@online{cranmerLagrangianNeuralNetworks2020,
  title = {Lagrangian {{Neural Networks}}},
  author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
  date = {2020-07-30},
  eprint = {2003.04630},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.04630},
  url = {http://arxiv.org/abs/2003.04630},
  urldate = {2025-01-03},
  abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Physics - Computational Physics,Physics - Data Analysis Statistics and Probability,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\53ZX6CWL\\Cranmer et al. - 2020 - Lagrangian Neural Networks.pdf;C\:\\Users\\eirik\\Zotero\\storage\\7AWSL36Q\\2003.html}
}

@article{eidnesPseudoHamiltonianNeuralNetworks2023,
  title = {Pseudo-{{Hamiltonian}} Neural Networks with State-Dependent External Forces},
  author = {Eidnes, Sølve and Stasik, Alexander J. and Sterud, Camilla and Bøhn, Eivind and Riemer-Sørensen, Signe},
  date = {2023-04},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {446},
  pages = {133673},
  issn = {01672789},
  doi = {10.1016/j.physd.2023.133673},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278923000271},
  urldate = {2024-10-07},
  abstract = {Hybrid machine learning based on Hamiltonian formulations has recently been successfully demonstrated for simple mechanical systems, both energy conserving and not energy conserving. We introduce a pseudo-Hamiltonian formulation that is a generalization of the Hamiltonian formulation via the port-Hamiltonian formulation, and show that pseudo-Hamiltonian neural network models can be used to learn external forces acting on a system. We argue that this property is particularly useful when the external forces are state dependent, in which case it is the pseudo-Hamiltonian structure that facilitates the separation of internal and external forces. Numerical results are provided for a forced and damped mass–spring system and a tank system of higher complexity, and a symmetric fourth-order integration scheme is introduced for improved training on sparse and noisy data.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\77MUNGZL\Eidnes et al. - 2023 - Pseudo-Hamiltonian neural networks with state-dependent external forces.pdf}
}

@article{eidnesPseudoHamiltonianNeuralNetworks2024,
  title = {Pseudo-{{Hamiltonian}} Neural Networks for Learning Partial Differential Equations},
  author = {Eidnes, Sølve and Lye, Kjetil Olsen},
  date = {2024-03},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {500},
  pages = {112738},
  issn = {00219991},
  doi = {10.1016/j.jcp.2023.112738},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999123008343},
  urldate = {2024-10-07},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\4DTFW3JZ\Eidnes og Lye - 2024 - Pseudo-Hamiltonian neural networks for learning partial differential equations.pdf}
}

@article{glorotUnderstandingDifficultyTraining,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\WANY9JV3\Glorot og Bengio - Understanding the difﬁculty of training deep feedforward neural networks.pdf}
}

@online{greydanusHamiltonianNeuralNetworks2019,
  title = {Hamiltonian {{Neural Networks}}},
  author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
  date = {2019-09-05},
  eprint = {1906.01563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.01563},
  urldate = {2024-10-07},
  abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\5JZZ2LHX\\Greydanus et al. - 2019 - Hamiltonian Neural Networks.pdf;C\:\\Users\\eirik\\Zotero\\storage\\C2N7TVUP\\1906.html}
}

@book{hairerGeometricNumericalIntegration2006,
  title = {Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations},
  shorttitle = {Geometric Numerical Integration},
  author = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  date = {2006},
  series = {Springer Series in Computational Mathematics},
  number = {31},
  publisher = {Springer},
  location = {Berlin},
  isbn = {978-3-540-30663-4},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\WJP9AFXQ\Hairer et al. - 2006 - Geometric numerical integration structure-preserving algorithms for ordinary differential equations.pdf}
}

@book{hairerSolvingOrdinaryDifferential1993,
  title = {Solving {{Ordinary Differential Equations I}}},
  author = {Hairer, Ernst and Wanner, Gerhard and Nørsett, Syvert P.},
  date = {1993},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  volume = {8},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78862-1},
  url = {http://link.springer.com/10.1007/978-3-540-78862-1},
  urldate = {2024-10-07},
  isbn = {978-3-540-56670-0 978-3-540-78862-1},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\6UVKQN5P\1993 - Solving Ordinary Differential Equations I.pdf}
}

@book{hairerSolvingOrdinaryDifferential1996,
  title = {Solving {{Ordinary Differential Equations II}}},
  author = {Hairer, Ernst and Wanner, Gerhard},
  date = {1996},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  volume = {14},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-05221-7},
  url = {http://link.springer.com/10.1007/978-3-642-05221-7},
  urldate = {2024-10-07},
  isbn = {978-3-642-05220-0 978-3-642-05221-7},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\7GZ9WFM4\Hairer og Wanner - 1996 - Solving Ordinary Differential Equations II.pdf}
}

@online{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-02-06},
  eprint = {1502.01852},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1502.01852},
  url = {http://arxiv.org/abs/1502.01852},
  urldate = {2025-01-13},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\HWCBNI6R\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf;C\:\\Users\\eirik\\Zotero\\storage\\XKC9VPDR\\1502.html}
}

@online{hoopCostAccuracyTradeOperatorLearning2022,
  title = {The {{Cost-Accuracy Trade-Off In Operator Learning With Neural Networks}}},
  author = {family=Hoop, given=Maarten V., prefix=de, useprefix=false and Huang, Daniel Zhengyu and Qian, Elizabeth and Stuart, Andrew M.},
  date = {2022-08-11},
  eprint = {2203.13181},
  eprinttype = {arXiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/2203.13181},
  urldate = {2024-10-21},
  abstract = {The term `surrogate modeling' in computational science and engineering refers to the development of computationally efficient approximations for expensive simulations, such as those arising from numerical solution of partial differential equations (PDEs). Surrogate modeling is an enabling methodology for many-query computations in science and engineering, which include iterative methods in optimization and sampling methods in uncertainty quantification. Over the last few years, several approaches to surrogate modeling for PDEs using neural networks have emerged, motivated by successes in using neural networks to approximate nonlinear maps in other areas. In principle, the relative merits of these different approaches can be evaluated by understanding, for each one, the cost required to achieve a given level of accuracy. However, the absence of a complete theory of approximation error for these approaches makes it difficult to assess this cost-accuracy trade-off. The purpose of the paper is to provide a careful numerical study of this issue, comparing a variety of different neural network architectures for operator approximation across a range of problems arising from PDE models in continuum mechanics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\YJJXF33P\\Hoop et al. - 2022 - The Cost-Accuracy Trade-Off In Operator Learning With Neural Networks.pdf;C\:\\Users\\eirik\\Zotero\\storage\\ZGQKCUVR\\2203.html}
}

@online{huangNormalizationTechniquesTraining2020,
  title = {Normalization {{Techniques}} in {{Training DNNs}}: {{Methodology}}, {{Analysis}} and {{Application}}},
  shorttitle = {Normalization {{Techniques}} in {{Training DNNs}}},
  author = {Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  date = {2020-09-27},
  eprint = {2009.12836},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.12836},
  url = {http://arxiv.org/abs/2009.12836},
  urldate = {2025-01-13},
  abstract = {Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\2TTJXPZT\\Huang et al. - 2020 - Normalization Techniques in Training DNNs Methodology, Analysis and Application.pdf;C\:\\Users\\eirik\\Zotero\\storage\\IU2JYP73\\2009.html}
}

@article{karniadakisPhysicsinformedMachineLearning2021,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  date = {2021-05-24},
  journaltitle = {Nature Reviews Physics},
  shortjournal = {Nat Rev Phys},
  volume = {3},
  number = {6},
  pages = {422--440},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  url = {https://www.nature.com/articles/s42254-021-00314-5},
  urldate = {2024-10-07},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\3993XCSP\Karniadakis et al. - 2021 - Physics-informed machine learning.pdf}
}

@online{kidgerEquinoxNeuralNetworks2021,
  title = {Equinox: Neural Networks in {{JAX}} via Callable {{PyTrees}} and Filtered Transformations},
  shorttitle = {Equinox},
  author = {Kidger, Patrick and Garcia, Cristian},
  date = {2021-10-30},
  eprint = {2111.00254},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.00254},
  urldate = {2024-10-28},
  abstract = {JAX and PyTorch are two popular Python autodifferentiation frameworks. JAX is based around pure functions and functional programming. PyTorch has popularised the use of an object-oriented (OO) class-based syntax for defining parameterised functions, such as neural networks. That this seems like a fundamental difference means current libraries for building parameterised functions in JAX have either rejected the OO approach entirely (Stax) or have introduced OO-to-functional transformations, multiple new abstractions, and been limited in the extent to which they integrate with JAX (Flax, Haiku, Objax). Either way this OO/functional difference has been a source of tension. Here, we introduce `Equinox', a small neural network library showing how a PyTorch-like class-based approach may be admitted without sacrificing JAX-like functional programming. We provide two main ideas. One: parameterised functions are themselves represented as `PyTrees', which means that the parameterisation of a function is transparent to the JAX framework. Two: we filter a PyTree to isolate just those components that should be treated when transforming (`jit', `grad' or `vmap'-ing) a higher-order function of a parameterised function -- such as a loss function applied to a model. Overall Equinox resolves the above tension without introducing any new programmatic abstractions: only PyTrees and transformations, just as with regular JAX. Equinox is available at \textbackslash url\{https://github.com/patrick-kidger/equinox\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\PZWRSMFT\\Kidger og Garcia - 2021 - Equinox neural networks in JAX via callable PyTrees and filtered transformations.pdf;C\:\\Users\\eirik\\Zotero\\storage\\QZYMC8WT\\2111.html}
}

@online{kossaifiLibraryLearningNeural2024,
  title = {A {{Library}} for {{Learning Neural Operators}}},
  author = {Kossaifi, Jean and Kovachki, Nikola and Li, Zongyi and Pitt, David and Liu-Schiaffini, Miguel and George, Robert Joseph and Bonev, Boris and Azizzadenesheli, Kamyar and Berner, Julius and Anandkumar, Anima},
  date = {2024-12-17},
  eprint = {2412.10354},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.10354},
  url = {http://arxiv.org/abs/2412.10354},
  urldate = {2024-12-29},
  abstract = {We present NeuralOperator, an open-source Python library for operator learning. Neural operators generalize neural networks to maps between function spaces instead of finite-dimensional Euclidean spaces. They can be trained and inferenced on input and output functions given at various discretizations, satisfying a discretization convergence properties. Built on top of PyTorch, NeuralOperator provides all the tools for training and deploying neural operator models, as well as developing new ones, in a high-quality, tested, open-source package. It combines cutting-edge models and customizability with a gentle learning curve and simple user interface for newcomers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\N8JJGIVQ\\Kossaifi et al. - 2024 - A Library for Learning Neural Operators.pdf;C\:\\Users\\eirik\\Zotero\\storage\\WI9D8GYF\\2412.html}
}

@online{kovachkiNeuralOperatorLearning2024,
  title = {Neural {{Operator}}: {{Learning Maps Between Function Spaces}}},
  shorttitle = {Neural {{Operator}}},
  author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  date = {2024-05-02},
  eprint = {2108.08481},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.5555/3648699.3648788},
  url = {http://arxiv.org/abs/2108.08481},
  urldate = {2024-10-21},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\QPGSWPGS\\Kovachki et al. - 2024 - Neural Operator Learning Maps Between Function Spaces.pdf;C\:\\Users\\eirik\\Zotero\\storage\\TKPSQTNB\\2108.html}
}

@online{kovachkiOperatorLearningAlgorithms2024,
  title = {Operator {{Learning}}: {{Algorithms}} and {{Analysis}}},
  shorttitle = {Operator {{Learning}}},
  author = {Kovachki, Nikola B. and Lanthaler, Samuel and Stuart, Andrew M.},
  date = {2024-02-23},
  eprint = {2402.15715},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2402.15715},
  urldate = {2024-10-07},
  abstract = {Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretical understanding of neural operators, focusing on an approximation theoretic point of view.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {C:\Users\eirik\Zotero\storage\8G35FFNP\Kovachki et al. - 2024 - Operator Learning Algorithms and Analysis.pdf}
}

@online{lanthalerOperatorLearningPCANet2023,
  title = {Operator Learning with {{PCA-Net}}: Upper and Lower Complexity Bounds},
  shorttitle = {Operator Learning with {{PCA-Net}}},
  author = {Lanthaler, Samuel},
  date = {2023-10-13},
  eprint = {2303.16317},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.16317},
  url = {http://arxiv.org/abs/2303.16317},
  urldate = {2025-01-03},
  abstract = {PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate operators between infinite-dimensional function spaces. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction: First, a novel universal approximation result is derived, under minimal assumptions on the underlying operator and the data-generating distribution. Then, two potential obstacles to efficient operator learning with PCA-Net are identified, and made precise through lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates to the inherent complexity of the space of operators between infinite-dimensional input and output spaces, resulting in a rigorous and quantifiable statement of a "curse of parametric complexity", an infinite-dimensional analogue of the well-known curse of dimensionality encountered in high-dimensional approximation problems. In addition to these lower bounds, upper complexity bounds are finally derived. A suitable smoothness criterion is shown to ensure an algebraic decay of the PCA eigenvalues. Furthermore, it is shown that PCA-Net can overcome the general curse for specific operators of interest, arising from the Darcy flow and the Navier-Stokes equations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\ZMC25GNT\\Lanthaler - 2023 - Operator learning with PCA-Net upper and lower complexity bounds.pdf;C\:\\Users\\eirik\\Zotero\\storage\\QVC5JZCD\\2303.html}
}

@book{leimkuhlerSimulatingHamiltonianDynamics2005,
  title = {Simulating {{Hamiltonian Dynamics}}},
  author = {Leimkuhler, Benedict and Reich, Sebastian},
  date = {2005-02-14},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511614118},
  url = {https://www.cambridge.org/core/product/identifier/9780511614118/type/book},
  urldate = {2024-10-07},
  isbn = {978-0-521-77290-7 978-0-511-61411-8},
  file = {C:\Users\eirik\Zotero\storage\BKCF96KS\Ciarlet et al. - 14 Simulating Hamiltonian Dynamics.pdf}
}

@online{leoniDeepONetPredictionLinear2021,
  title = {{{DeepONet}} Prediction of Linear Instability Waves in High-Speed Boundary Layers},
  author = {Leoni, P. Clark Di and Lu, L. and Meneveau, C. and Karniadakis, G. and Zaki, T. A.},
  date = {2021-05-18},
  eprint = {2105.08697},
  eprinttype = {arXiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2105.08697},
  urldate = {2024-10-28},
  abstract = {Deep operator networks (DeepONets) are trained to predict the linear amplification of instability waves in high-speed boundary layers and to perform data assimilation. In contrast to traditional networks that approximate functions, DeepONets are designed to approximate operators. Using this framework, we train a DeepONet to take as inputs an upstream disturbance and a downstream location of interest, and to provide as output the perturbation field downstream in the boundary layer. DeepONet thus approximates the linearized and parabolized Navier-Stokes operator for this flow. Once trained, the network can perform predictions of the downstream flow for a wide variety of inflow conditions, without the need to calculate the whole trajectory of the perturbations, and at a very small computational cost compared to discretization of the original equations. In addition, we show that DeepONets can solve the inverse problem, where downstream wall measurements are adopted as input and a trained network can predict the upstream disturbances that led to these observations. This capability, along with the forward predictions, allows us to perform a full data assimilation cycle: starting from wall-pressure data, we predict the upstream disturbance using the inverse DeepONet and its evolution using the forward DeepONet.},
  pubstate = {prepublished},
  keywords = {Physics - Fluid Dynamics},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\VSIVY9X3\\Leoni et al. - 2021 - DeepONet prediction of linear instability waves in high-speed boundary layers.pdf;C\:\\Users\\eirik\\Zotero\\storage\\4RM9N6SP\\2105.html}
}

@online{liFourierNeuralOperator2021,
  title = {Fourier {{Neural Operator}} for {{Parametric Partial Differential Equations}}},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  date = {2021-05-16},
  eprint = {2010.08895},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2010.08895},
  urldate = {2024-10-07},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\BMWPJ85B\\Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Differential Equations.pdf;C\:\\Users\\eirik\\Zotero\\storage\\YJMJY8HJ\\2010.html}
}

@online{liNeuralOperatorGraph2020,
  title = {Neural {{Operator}}: {{Graph Kernel Network}} for {{Partial Differential Equations}}},
  shorttitle = {Neural {{Operator}}},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  date = {2020-03-07},
  eprint = {2003.03485},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.03485},
  urldate = {2024-10-21},
  abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\8F656KDU\\Li et al. - 2020 - Neural Operator Graph Kernel Network for Partial Differential Equations.pdf;C\:\\Users\\eirik\\Zotero\\storage\\V78XGZVG\\2003.html}
}

@online{liPhysicsInformedNeuralOperator2023,
  title = {Physics-{{Informed Neural Operator}} for {{Learning Partial Differential Equations}}},
  author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  date = {2023-07-29},
  eprint = {2111.03794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.03794},
  url = {http://arxiv.org/abs/2111.03794},
  urldate = {2024-12-10},
  abstract = {In this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization-convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, e.g., in multi-scale dynamic systems such as Kolmogorov flows.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\E32F7JH7\\Li et al. - 2023 - Physics-Informed Neural Operator for Learning Partial Differential Equations.pdf;C\:\\Users\\eirik\\Zotero\\storage\\HLSVKJS8\\2111.html}
}

@article{luComprehensiveFairComparison2022,
  title = {A Comprehensive and Fair Comparison of Two Neural Operators (with Practical Extensions) Based on {{FAIR}} Data},
  author = {Lu, Lu and Meng, Xuhui and Cai, Shengze and Mao, Zhiping and Goswami, Somdatta and Zhang, Zhongqiang and Karniadakis, George Em},
  date = {2022-04},
  journaltitle = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {393},
  pages = {114778},
  issn = {00457825},
  doi = {10.1016/j.cma.2022.114778},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782522001207},
  urldate = {2024-10-28},
  abstract = {Neural operators can learn nonlinear mappings between function spaces and offer a new simulation paradigm for real-time prediction of complex dynamics for realistic diverse applications as well as for system identification in science and engineering. Herein, we investigate the performance of two neural operators, which have shown promising results so far, and we develop new practical extensions that will make them more accurate and robust and importantly more suitable for industrial-complexity applications. The first neural operator, DeepONet, was published in 2019 (Lu et al., 2019), and its original architecture was based on the universal approximation theorem of Chen \& Chen (1995). The second one, named Fourier Neural Operator or FNO, was published in 2020 (Li et al., 2020), and it is based on parameterizing the integral kernel in the Fourier space. DeepONet is represented by a summation of products of neural networks (NNs), corresponding to the branch NN for the input function and the trunk NN for the output function; both NNs are general architectures, e.g., the branch NN can be replaced with a CNN or a ResNet. According to Kovachki et al. (2021), FNO in its continuous form can be viewed conceptually as a DeepONet with a specific architecture of the branch NN and a trunk NN represented by a trigonometric basis. In order to compare FNO with DeepONet computationally for realistic setups, we develop several extensions of FNO that can deal with complex geometric domains as well as mappings where the input and output function spaces are of different dimensions. We also develop an extended DeepONet with special features that provide inductive bias and accelerate training, and we present a faster implementation of DeepONet with cost comparable to the computational cost of FNO, which is based on the Fast Fourier Transform.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\73M8SVAJ\Lu et al. - 2022 - A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAI.pdf}
}

@article{luDeepONetLearningNonlinear2021,
  title = {{{DeepONet}}: {{Learning}} Nonlinear Operators for Identifying Differential Equations Based on the Universal Approximation Theorem of Operators},
  shorttitle = {{{DeepONet}}},
  author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  date = {2021-03-18},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {3},
  number = {3},
  eprint = {1910.03193},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {218--229},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00302-5},
  url = {http://arxiv.org/abs/1910.03193},
  urldate = {2024-10-07},
  abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator [5]. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors xi, i = 1, . . . , m (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\eirik\Zotero\storage\LAC7WYX4\Lu et al. - 2021 - DeepONet Learning nonlinear operators for identifying differential equations based on the universal.pdf}
}

@article{mcclennySelfAdaptivePhysicsInformedNeural2023a,
  title = {Self-{{Adaptive Physics-Informed Neural Networks}} Using a {{Soft Attention Mechanism}}},
  author = {McClenny, Levi and Braga-Neto, Ulisses},
  date = {2023-02},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {474},
  eprint = {2009.04544},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {111722},
  issn = {00219991},
  doi = {10.1016/j.jcp.2022.111722},
  url = {http://arxiv.org/abs/2009.04544},
  urldate = {2024-12-27},
  abstract = {Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of "stiff" PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. In addition, we show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\LS2MKI28\\McClenny og Braga-Neto - 2023 - Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism.pdf;C\:\\Users\\eirik\\Zotero\\storage\\7KKCCCQZ\\2009.html}
}

@online{michalowskaDONLSTMMultiResolutionLearning2023,
  title = {{{DON-LSTM}}: {{Multi-Resolution Learning}} with {{DeepONets}} and {{Long Short-Term Memory Neural Networks}}},
  shorttitle = {{{DON-LSTM}}},
  author = {Michałowska, Katarzyna and Goswami, Somdatta and Karniadakis, George Em and Riemer-Sørensen, Signe},
  date = {2023-10-03},
  eprint = {2310.02491},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.02491},
  urldate = {2024-10-14},
  abstract = {Deep operator networks (DeepONets, DONs) offer a distinct advantage over traditional neural networks in their ability to be trained on multi-resolution data. This property becomes especially relevant in real-world scenarios where highresolution measurements are difficult to obtain, while low-resolution data is more readily available. Nevertheless, DeepONets alone often struggle to capture and maintain dependencies over long sequences compared to other state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM, which extends the DeepONet with a long short-term memory network (LSTM). Combining these two architectures, we equip the network with explicit mechanisms to leverage multi-resolution data, as well as capture temporal dependencies in long sequences. We test our method on long-time-evolution modeling of multiple non-linear systems and show that the proposed multi-resolution DON-LSTM achieves significantly lower generalization error and requires fewer high-resolution samples compared to its vanilla counterparts.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\eirik\Zotero\storage\Z4FFJWAB\Michałowska et al. - 2023 - DON-LSTM Multi-Resolution Learning with DeepONets and Long Short-Term Memory Neural Networks.pdf}
}

@book{olverApplicationsLieGroups1986,
  title = {Applications of {{Lie Groups}} to {{Differential Equations}}},
  author = {Olver, Peter J.},
  date = {1986},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {107},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4684-0274-2},
  url = {https://link.springer.com/10.1007/978-1-4684-0274-2},
  urldate = {2025-01-13},
  isbn = {978-1-4684-0276-6 978-1-4684-0274-2},
  langid = {english}
}

@article{pinkusApproximationTheoryMLP1999,
  title = {Approximation Theory of the {{MLP}} Model in Neural Networks},
  author = {Pinkus, Allan},
  date = {1999-01},
  journaltitle = {Acta Numerica},
  shortjournal = {Acta Numerica},
  volume = {8},
  pages = {143--195},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492900002919},
  url = {https://www.cambridge.org/core/product/identifier/S0962492900002919/type/journal_article},
  urldate = {2024-10-19},
  abstract = {In this survey we discuss various approximation-theoretic problems that arise in the multilayer feedforward perceptron (MLP) model in neural networks. The MLP model is one of the more popular and practical of the many neural network models. Mathematically it is also one of the simpler models. Nonetheless the mathematics of this model is not well understood, and many of these problems are approximation-theoretic in character. Most of the research we will discuss is of very recent vintage. We will report on what has been done and on various unanswered questions. We will not be presenting practical (algorithmic) methods. We will, however, be exploring the capabilities and limitations of this model.},
  langid = {english}
}

@online{prasthoferVariableInputDeepOperator2022,
  title = {Variable-{{Input Deep Operator Networks}}},
  author = {Prasthofer, Michael and Ryck, Tim De and Mishra, Siddhartha},
  date = {2022-05-23},
  eprint = {2205.11404},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.11404},
  url = {http://arxiv.org/abs/2205.11404},
  urldate = {2024-12-18},
  abstract = {Existing architectures for operator learning require that the number and locations of sensors (where the input functions are evaluated) remain the same across all training and test samples, significantly restricting the range of their applicability. We address this issue by proposing a novel operator learning framework, termed Variable-Input Deep Operator Network (VIDON), which allows for random sensors whose number and locations can vary across samples. VIDON is invariant to permutations of sensor locations and is proved to be universal in approximating a class of continuous operators. We also prove that VIDON can efficiently approximate operators arising in PDEs. Numerical experiments with a diverse set of PDEs are presented to illustrate the robust performance of VIDON in learning operators.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\D8Q9CG3U\\Prasthofer et al. - 2022 - Variable-Input Deep Operator Networks.pdf;C\:\\Users\\eirik\\Zotero\\storage\\T7HASDW6\\2205.html}
}

@article{raissiDeepHiddenPhysics,
  title = {Deep {{Hidden Physics Models}}: {{Deep Learning}} of {{Nonlinear Partial Diﬀerential Equations}}},
  author = {Raissi, Maziar},
  abstract = {We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers’, Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr¨odinger, and Navier-Stokes equations.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\ZXMK7Y5G\Raissi - Deep Hidden Physics Models Deep Learning of Nonlinear Partial Diﬀerential Equations.pdf}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  date = {2019-02},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.10.045},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
  urldate = {2024-10-07},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\49KDWZWU\Raissi et al. - 2019 - Physics-informed neural networks A deep learning framework for solving forward and inverse problems.pdf}
}

@online{seidmanNOMADNonlinearManifold2022,
  title = {{{NOMAD}}: {{Nonlinear Manifold Decoders}} for {{Operator Learning}}},
  shorttitle = {{{NOMAD}}},
  author = {Seidman, Jacob H. and Kissas, Georgios and Perdikaris, Paris and Pappas, George J.},
  date = {2022-06-07},
  eprint = {2206.03551},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.03551},
  url = {http://arxiv.org/abs/2206.03551},
  urldate = {2024-12-18},
  abstract = {Supervised learning in function spaces is an emerging area of machine learning research with applications to the prediction of complex physical systems such as fluid flows, solid mechanics, and climate modeling. By directly learning maps (operators) between infinite dimensional function spaces, these models are able to learn discretization invariant representations of target functions. A common approach is to represent such target functions as linear combinations of basis elements learned from data. However, there are simple scenarios where, even though the target functions form a low dimensional submanifold, a very large number of basis elements is needed for an accurate linear representation. Here we present NOMAD, a novel operator learning framework with a nonlinear decoder map capable of learning finite dimensional representations of nonlinear submanifolds in function spaces. We show this method is able to accurately learn low dimensional representations of solution manifolds to partial differential equations while outperforming linear models of larger size. Additionally, we compare to state-of-the-art operator learning methods on a complex fluid dynamics benchmark and achieve competitive performance with a significantly smaller model size and training cost.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\N3G9S8GI\\Seidman et al. - 2022 - NOMAD Nonlinear Manifold Decoders for Operator Learning.pdf;C\:\\Users\\eirik\\Zotero\\storage\\JUBPYDP9\\2206.html}
}

@online{tanakaNeuralOperatorsMeet2024,
  title = {Neural {{Operators Meet Energy-based Theory}}: {{Operator Learning}} for {{Hamiltonian}} and {{Dissipative PDEs}}},
  shorttitle = {Neural {{Operators Meet Energy-based Theory}}},
  author = {Tanaka, Yusuke and Yaguchi, Takaharu and Iwata, Tomoharu and Ueda, Naonori},
  date = {2024-02-14},
  eprint = {2402.09018},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2402.09018},
  urldate = {2024-10-07},
  abstract = {The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs. Experiments on multiple physical systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\eirik\Zotero\storage\3BGBHVUH\Tanaka et al. - 2024 - Neural Operators Meet Energy-based Theory Operator Learning for Hamiltonian and Dissipative PDEs.pdf}
}

@article{tianpingchenUniversalApproximationNonlinear1995,
  title = {Universal Approximation to Nonlinear Operators by Neural Networks with Arbitrary Activation Functions and Its Application to Dynamical Systems},
  author = {{Tianping Chen} and {Hong Chen}},
  date = {1995-07},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {6},
  number = {4},
  pages = {911--917},
  issn = {10459227},
  doi = {10.1109/72.392253},
  url = {http://ieeexplore.ieee.org/document/392253/},
  urldate = {2024-12-18}
}

@article{wangImprovedArchitecturesTraining2022a,
  title = {Improved {{Architectures}} and {{Training Algorithms}} for {{Deep Operator Networks}}},
  author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  date = {2022-08},
  journaltitle = {Journal of Scientific Computing},
  shortjournal = {J Sci Comput},
  volume = {92},
  number = {2},
  pages = {35},
  issn = {0885-7474, 1573-7691},
  doi = {10.1007/s10915-022-01881-0},
  url = {https://link.springer.com/10.1007/s10915-022-01881-0},
  urldate = {2024-10-27},
  abstract = {Operator learning techniques have recently emerged as a powerful tool for learning maps between infinite-dimensional Banach spaces. Trained under appropriate constraints, they can also be effective in learning the solution operator of partial differential equations (PDEs) in an entirely self-supervised manner. In this work we analyze the training dynamics of deep operator networks (DeepONets) through the lens of Neural Tangent Kernel theory, and reveal a bias that favors the approximation of functions with larger magnitudes. To correct this bias we propose to adaptively re-weight the importance of each training example, and demonstrate how this procedure can effectively balance the magnitude of back-propagated gradients during training via gradient descent. We also propose a novel network architecture that is more resilient to vanishing gradient pathologies. Taken together, our developments provide new insights into the training of DeepONets and consistently improve their predictive accuracy by a factor of 10-50x, demonstrated in the challenging setting of learning PDE solution operators in the absence of paired input-output observations. All code and data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/ ImprovedDeepONets.},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\56YYS86J\Wang et al. - 2022 - Improved Architectures and Training Algorithms for Deep Operator Networks.pdf}
}

@online{wangLearningSolutionOperator2021,
  title = {Learning the Solution Operator of Parametric Partial Differential Equations with Physics-Informed {{DeepOnets}}},
  author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  date = {2021-03-19},
  eprint = {2103.10974},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.10974},
  url = {http://arxiv.org/abs/2103.10974},
  urldate = {2025-01-03},
  abstract = {Deep operator networks (DeepONets) are receiving increased attention thanks to their demonstrated capability to approximate nonlinear operators between infinite-dimensional Banach spaces. However, despite their remarkable early promise, they typically require large training data-sets consisting of paired input-output observations which may be expensive to obtain, while their predictions may not be consistent with the underlying physical principles that generated the observed data. In this work, we propose a novel model class coined as physics-informed DeepONets, which introduces an effective regularization mechanism for biasing the outputs of DeepOnet models towards ensuring physical consistency. This is accomplished by leveraging automatic differentiation to impose the underlying physical laws via soft penalty constraints during model training. We demonstrate that this simple, yet remarkably effective extension can not only yield a significant improvement in the predictive accuracy of DeepOnets, but also greatly reduce the need for large training data-sets. To this end, a remarkable observation is that physics-informed DeepONets are capable of solving parametric partial differential equations (PDEs) without any paired input-output observations, except for a set of given initial or boundary conditions. We illustrate the effectiveness of the proposed framework through a series of comprehensive numerical studies across various types of PDEs. Strikingly, a trained physics informed DeepOnet model can predict the solution of \$\textbackslash mathcal\{O\}(10\textasciicircum 3)\$ time-dependent PDEs in a fraction of a second -- up to three orders of magnitude faster compared a conventional PDE solver. The data and code accompanying this manuscript are publicly available at \textbackslash url\{https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\TM4TJMQN\\Wang et al. - 2021 - Learning the solution operator of parametric partial differential equations with physics-informed De.pdf;C\:\\Users\\eirik\\Zotero\\storage\\6SMACKHT\\2103.html}
}

@online{wangLongtimeIntegrationParametric2021,
  title = {Long-Time Integration of Parametric Evolution Equations with Physics-Informed {{DeepONets}}},
  author = {Wang, Sifan and Perdikaris, Paris},
  date = {2021-06-09},
  eprint = {2106.05384},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.05384},
  url = {http://arxiv.org/abs/2106.05384},
  urldate = {2025-01-13},
  abstract = {Ordinary and partial differential equations (ODEs/PDEs) play a paramount role in analyzing and simulating complex dynamic processes across all corners of science and engineering. In recent years machine learning tools are aspiring to introduce new effective ways of simulating PDEs, however existing approaches are not able to reliably return stable and accurate predictions across long temporal horizons. We aim to address this challenge by introducing an effective framework for learning infinite-dimensional operators that map random initial conditions to associated PDE solutions within a short time interval. Such latent operators can be parametrized by deep neural networks that are trained in an entirely self-supervised manner without requiring any paired input-output observations. Global long-time predictions across a range of initial conditions can be then obtained by iteratively evaluating the trained model using each prediction as the initial condition for the next evaluation step. This introduces a new approach to temporal domain decomposition that is shown to be effective in performing accurate long-time simulations for a wide range of parametric ODE and PDE systems, from wave propagation, to reaction-diffusion dynamics and stiff chemical kinetics, all at a fraction of the computational cost needed by classical numerical solvers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\CVJNI6MQ\\Wang og Perdikaris - 2021 - Long-time integration of parametric evolution equations with physics-informed DeepONets.pdf;C\:\\Users\\eirik\\Zotero\\storage\\NX7H9T5L\\2106.html}
}

@article{xiongKoopmanNeuralOperator2024,
  title = {Koopman Neural Operator as a Mesh-Free Solver of Non-Linear Partial Differential Equations},
  author = {Xiong, Wei and Huang, Xiaomeng and Zhang, Ziyang and Deng, Ruixuan and Sun, Pei and Tian, Yang},
  date = {2024-09},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {513},
  pages = {113194},
  issn = {00219991},
  doi = {10.1016/j.jcp.2024.113194},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999124004431},
  urldate = {2025-01-03},
  langid = {english},
  file = {C:\Users\eirik\Zotero\storage\YK3Q24ZI\Xiong et al. - 2024 - Koopman neural operator as a mesh-free solver of non-linear partial differential equations.pdf}
}

@online{xuPreprocessingPhysicsinformedNeural2024,
  title = {On the {{Preprocessing}} of {{Physics-informed Neural Networks}}: {{How}} to {{Better Utilize Data}} in {{Fluid Mechanics}}},
  shorttitle = {On the {{Preprocessing}} of {{Physics-informed Neural Networks}}},
  author = {Xu, Shengfeng and Yan, Chang and Sun, Zhenxu and Huang, Renfang and Guo, Dilong and Yang, Guowei},
  date = {2024-07-11},
  eprint = {2403.19923},
  eprinttype = {arXiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.2403.19923},
  url = {http://arxiv.org/abs/2403.19923},
  urldate = {2025-01-07},
  abstract = {Physics-Informed Neural Networks (PINNs) serve as a flexible alternative for tackling forward and inverse problems in differential equations, displaying impressive advancements in diverse areas of applied mathematics. Despite integrating both data and underlying physics to enrich the neural network's understanding, concerns regarding the effectiveness and practicality of PINNs persist. Over the past few years, extensive efforts in the current literature have been made to enhance this evolving method, by drawing inspiration from both machine learning algorithms and numerical methods. Despite notable progressions in PINNs algorithms, the important and fundamental field of data preprocessing remain unexplored, limiting the applications of PINNs especially in solving inverse problems. Therefore in this paper, a concise yet potent data preprocessing method focusing on data normalization was proposed. By applying a linear transformation to both the data and corresponding equations concurrently, the normalized PINNs approach was evaluated on the task of reconstructing flow fields in three turbulent cases. The results illustrate that by adhering to the data preprocessing procedure, PINNs can robustly achieve higher prediction accuracy for all flow quantities under different hyperparameter setups, without incurring extra computational cost, distinctly improving the utilization of limited training data. Though only verified in Navier-Stokes (NS) equations, this method holds potential for application to various other equations.},
  pubstate = {prepublished},
  keywords = {Physics - Fluid Dynamics},
  file = {C\:\\Users\\eirik\\Zotero\\storage\\EAUL7H9B\\Xu et al. - 2024 - On the Preprocessing of Physics-informed Neural Networks How to Better Utilize Data in Fluid Mechan.pdf;C\:\\Users\\eirik\\Zotero\\storage\\RANA5IE3\\2403.html}
}

@software{zotero-92,
  type = {software}
}
